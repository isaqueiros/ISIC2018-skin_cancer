{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DWRU0umB0B9"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dafDhYI3B8nR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gew3IKBPH_31"
      },
      "source": [
        "## Data Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubc6lCVLIBKR"
      },
      "outputs": [],
      "source": [
        "class DataReader:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  '''Capture of csv datasets as pandas dataframes and inclusion of image path for each record'''\n",
        "  def capture_df(self, df_type):\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks/skin-cancer-project/'\n",
        "\n",
        "    if df_type==\"train\":\n",
        "      csv_path = 'datasets/train/'\n",
        "      csv_name = 'ISIC2018_Task3_Training_GroundTruth.csv'\n",
        "    elif df_type == \"validate\":\n",
        "      csv_path = 'datasets/validate/'\n",
        "      csv_name = 'ISIC2018_Task3_Validation_GroundTruth.csv'\n",
        "    else:\n",
        "      csv_path = 'datasets/test/'\n",
        "      csv_name = 'ISIC2018_Task3_Test_GroundTruth.csv'\n",
        "\n",
        "    df = pd.read_csv(base_path + csv_path + csv_name)\n",
        "    df['img_path'] = base_path + 'images/' + df['image']+'.jpg'\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7IQt58rB9IS"
      },
      "source": [
        "## Data Balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BJxMOtobB2XJ"
      },
      "outputs": [],
      "source": [
        "class DataBalancer:\n",
        "  RANDOM_STATE = 42\n",
        "\n",
        "  def __init__(self, balance_target, classes):\n",
        "    self.BALANCE_COUNT_PER_CLASS = balance_target\n",
        "    self.CLASSES = classes\n",
        "\n",
        "  '''Balance the received dataset, upsampling and undersampling as required to achieve target count of records per class'''\n",
        "  def balance_data(self, df):\n",
        "\n",
        "    df['label'] = df[self.CLASSES].idxmax(axis=1)\n",
        "    df_balanced = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    for label in self.CLASSES:\n",
        "        df_class = df[df['label'] == label]\n",
        "        current_count = len(df_class)\n",
        "\n",
        "        if current_count < self.BALANCE_COUNT_PER_CLASS:\n",
        "            # Upsample\n",
        "            df_class_balanced = resample(df_class, replace=True, n_samples=self.BALANCE_COUNT_PER_CLASS, random_state=self.RANDOM_STATE)\n",
        "        else:\n",
        "            # Downsample\n",
        "            df_class_balanced = resample(df_class, replace=False, n_samples=self.BALANCE_COUNT_PER_CLASS, random_state=self.RANDOM_STATE)\n",
        "\n",
        "        df_balanced = pd.concat([df_balanced, df_class_balanced])\n",
        "\n",
        "    df = df_balanced.sample(frac=1, random_state=self.RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNQ3l6zpFlfT"
      },
      "source": [
        "## Data Preparation Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mP9oOHZD-i0"
      },
      "outputs": [],
      "source": [
        "class DataPreparer:\n",
        "\n",
        "  def __init__(self, aug_rotation, batch_size, channels, image_resize, classes):\n",
        "    self.AUGMENTATION_ROTATION = aug_rotation\n",
        "    self.BATCH_SIZE = batch_size\n",
        "    self.CHANNELS = channels\n",
        "    self.IMAGE_RESIZE = image_resize\n",
        "    self.CLASSES = classes\n",
        "    self.augmenter = keras.Sequential([keras.layers.RandomRotation(factor=self.AUGMENTATION_ROTATION),])\n",
        "\n",
        "  '''Implementation of preprocessing techniques to include data augmentation, image resizing and image pixel value normalisation'''\n",
        "  def load_and_preprocess_image(self, path, label, training=False):\n",
        "      image = tf.io.read_file(path)\n",
        "      image = tf.image.decode_jpeg(image, channels=self.CHANNELS)\n",
        "\n",
        "      if training:\n",
        "          image = self.augmenter(image)\n",
        "\n",
        "      image = tf.image.resize(image, self.IMAGE_RESIZE)\n",
        "      image = image / 255.0\n",
        "      return image, label\n",
        "\n",
        "  '''Creation of tensor dataset, mapping labels and images, and setting batches'''\n",
        "  def create_dataset(self, df, training=False):\n",
        "    image_path = df['img_path'].values\n",
        "    labels = df[self.CLASSES].values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_path, labels))\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: self.load_and_preprocess_image(x, y, training),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "      )\n",
        "\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(512)\n",
        "\n",
        "    dataset = dataset.batch(self.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
