{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "gJJt3ZtyK5ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook consists of different data preprocessing options that will be used to explore the best flow for the image classification task of diagnosis of skin lesions with multi-class classification through dermoscopic images."
      ],
      "metadata": {
        "id": "gDazeblLL3bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries Import"
      ],
      "metadata": {
        "id": "vs34QQYmOeMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as enPP\n",
        "from tensorflow.keras.applications.resnet import preprocess_input as rnPP\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as inPP\n",
        "from tensorflow.keras.applications.densenet import preprocess_input as dnPP"
      ],
      "metadata": {
        "id": "d6mBotL9OgPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Balancing"
      ],
      "metadata": {
        "id": "MexZNYIZMJ9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em33JpelKtIP"
      },
      "outputs": [],
      "source": [
        "class DataBalancer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def oversampler(self, target, df):\n",
        "    oversampled_groups = []\n",
        "\n",
        "    for label, group in df.groupby('label_encoded'):\n",
        "      # Calculate need for full copies and extra samples to reach target\n",
        "      copies = target // len(group)\n",
        "      extra_samples = target % len(group)\n",
        "\n",
        "      # Create full copies and extra samples, and concat them into a single df\n",
        "      duplicated = pd.concat([group] * copies)\n",
        "      sampled = group.sample(extra_samples, replace=True)\n",
        "      oversampled_group = pd.concat([duplicated, sampled])\n",
        "      oversampled_groups.append(oversampled_group)\n",
        "\n",
        "    df_oversampled = pd.concat(oversampled_groups).reset_index(drop=True)\n",
        "\n",
        "    return df_oversampled\n",
        "\n",
        "  def buildDS(self,df):\n",
        "    # Define base dataset\n",
        "    file_paths = df['img_path'].values\n",
        "    labels = df['label_encoded'].values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "    return ds\n",
        "\n",
        "  def buildWeightedDS(self,df):\n",
        "    # Define base dataset\n",
        "    file_paths = df['img_path'].values\n",
        "    labels = df['label_encoded'].values\n",
        "    weights = df['sample_weights'].values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels, weights))\n",
        "    return ds\n",
        "\n",
        "  '''\n",
        "  Capture an x number of images from each class based on smallest class number,\n",
        "  followed by oversampling to generate a sufficient number of images for training.\n",
        "  '''\n",
        "  def minBalancing(self, df):\n",
        "    # Identify smallest count per category\n",
        "    smallest_count = df.value_counts(\"lesion_type\").min()\n",
        "    # Sample based on smallest count\n",
        "    df = df.groupby('lesion_type').sample(n=smallest_count, random_state=42)\n",
        "\n",
        "    # Define base dataset\n",
        "    base_ds = self.buildDS(df)\n",
        "\n",
        "    # Perform oversampling (min 300 images per category)\n",
        "    target = 300\n",
        "    df_oversampled = self.oversampler(target, df)\n",
        "\n",
        "    # Define final dataset as base dataset + oversampled images\n",
        "    final_ds = self.buildDS(df_oversampled)\n",
        "\n",
        "    return final_ds\n",
        "\n",
        "  '''\n",
        "  Based on class with highest number of records, oversample other classes to match.\n",
        "  '''\n",
        "  def maxBalancing(self, df):\n",
        "    # Identify largest count per category\n",
        "    largest_count = df.value_counts(\"lesion_type\").max()\n",
        "\n",
        "    # Define base dataset\n",
        "    base_ds = self.buildDS(df)\n",
        "\n",
        "    # Perform oversampling (based on highest volume per category)\n",
        "    df_oversampled = self.oversampler(largest_count, df)\n",
        "\n",
        "    # Define final dataset as base dataset + oversampled images\n",
        "    final_ds = self.buildDS(df_oversampled)\n",
        "\n",
        "    return final_ds\n",
        "\n",
        "  '''\n",
        "  Calculate a weight for each class based on inverse frequency.\n",
        "  '''\n",
        "  def classWeightening(self, df):\n",
        "    # Get labels and calculate weights\n",
        "    labels = df['label_encoded'].values\n",
        "    class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                         classes=np.unique(labels), y=labels)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Map each record to it's correspondent weight based on class\n",
        "    weights = [class_weight_dict[label] for label in labels]\n",
        "    df['weights'] = weights\n",
        "\n",
        "    # Define weighted dataset\n",
        "    final_ds = self.buildWeightedDS(df)\n",
        "\n",
        "    return final_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "This section includes image resizing, normalisation, data augmentation techniques and"
      ],
      "metadata": {
        "id": "8SfFfepQNfKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreparer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def labelPathMapper(self, df, dataset):\n",
        "    # Convert labels columns to a single column\n",
        "    lesion_types = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
        "    df['lesion_type'] = df[lesion_types].idxmax(axis=1)\n",
        "    df = df.drop(columns=lesion_types)\n",
        "\n",
        "    # Encode categorical labels\n",
        "    label_lookup = {label: idx for idx, label in enumerate(df['lesion_type'].unique())}\n",
        "    df['label_encoded'] = df['lesion_type'].map(label_lookup)\n",
        "\n",
        "    # Create full drive path to each image\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks/skin-cancer-project/datasets/'+dataset+'/images/'\n",
        "    df['img_path'] = base_path + df['image']+'.jpg'\n",
        "\n",
        "    return df\n",
        "\n",
        "  def imageResizer(self, path, label, height, width):\n",
        "    image = tf.io.read_file(path)\n",
        "    image_decode = tf.image.decode_jpeg(image, channels=3)\n",
        "    resize_image = tf.image.resize(image_decode, [height, width])\n",
        "    return resize_image,label\n",
        "\n",
        "  def pixelNormalizer(self, image,label):\n",
        "    normalized_image = tf.cast(image/255. , tf.float32)\n",
        "    return normalized_image,label\n",
        "\n",
        "  def randomAugmentation(self):\n",
        "\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        layers.RandomShear(x_factor=0.2, y_factor=0.2),\n",
        "        ])\n",
        "\n",
        "    return data_augmentation"
      ],
      "metadata": {
        "id": "Cq_cHLnfNebJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Requirements Processing"
      ],
      "metadata": {
        "id": "E_GZNvrS7qeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSpecificProcessor:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def efficientNetPrep(self, image, label):\n",
        "    preprocessed_image = enPP(image)\n",
        "    return preprocessed_image, label\n",
        "\n",
        "  def resNetPrep(self, image, label):\n",
        "    preprocessed_image = rnPP(image)\n",
        "    return preprocessed_image, label\n",
        "\n",
        "  def inceptionPrep(self, image, label):\n",
        "    preprocessed_image = inPP(image)\n",
        "    return preprocessed_image, label\n",
        "\n",
        "  def denseNetPrep(self, image, label):\n",
        "    preprocessed_image = dnPP(image)\n",
        "    return preprocessed_image, label"
      ],
      "metadata": {
        "id": "29b-s_ji7vTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}